{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlGIZIqS5fT9",
        "outputId": "b958d1e9-1138-4d28-9495-c4ad10273bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.9-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.10)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "INFO: pip is looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-generativeai\n",
            "  Downloading google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
            "  Downloading google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.7.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Downloading google_generativeai-0.7.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: pip is still looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_generativeai-0.7.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.6.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading google_generativeai-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.4.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.3.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading google_generativeai-0.2.2-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.177.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: filetype, python-dotenv, pypdfium2, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, pydantic-settings, pdfminer.six, dataclasses-json, pdfplumber, langchain-google-genai, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.11.0.post1 filetype-1.2.0 httpx-sse-0.4.1 langchain-community-0.3.27 langchain-google-genai-2.0.10 marshmallow-3.26.1 mypy-extensions-1.1.0 pdfminer.six-20250506 pdfplumber-0.11.7 pydantic-settings-2.10.1 pypdfium2-4.30.0 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community langchain-google-genai google-generativeai pdfplumber faiss-cpu tiktoken PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import fitz  # PyMuPDF for link extraction\n",
        "import numpy as np\n",
        "import pdfplumber\n",
        "from typing import List, Tuple\n",
        "from sklearn.preprocessing import normalize\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "7E0H46qK8An4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY = \"Google API Key\"\n",
        "assert GOOGLE_API_KEY, \"Set your GOOGLE_API_KEY as an environment variable\""
      ],
      "metadata": {
        "id": "Bqp6Ni0z7dA6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load and extract text from resume PDF ===\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "sloy6_C476JT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Extract text + URL pairs from PDF ===\n",
        "def extract_links_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    links = []\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc[page_num]\n",
        "        for link in page.get_links():\n",
        "            if link['uri']:  # It's an external URL\n",
        "                rect = link['from']\n",
        "                try:\n",
        "                    text = page.get_textbox(rect)\n",
        "                    if text.strip():\n",
        "                        links.append((text.strip(), link['uri']))\n",
        "                except:\n",
        "                    continue\n",
        "    return links  # list of (text, url)"
      ],
      "metadata": {
        "id": "s8MHMvhNBBEP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Chunk text into Document objects ===\n",
        "def chunk_text_with_metadata(text, links, chunk_size=800, chunk_overlap=100):\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    chunks = splitter.create_documents([text])\n",
        "\n",
        "    # Attach links as global metadata to each chunk\n",
        "    for chunk in chunks:\n",
        "        chunk.metadata[\"links\"] = links\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "ltCqUicDBFtD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Chunking the text ===\n",
        "def chunk_text(text, chunk_size=800, chunk_overlap=100):\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    return splitter.create_documents([text])"
      ],
      "metadata": {
        "id": "rU37l-bn8GW5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Embed documents with Gemini in batches (with rate limit) ===\n",
        "def embed_and_normalize_documents(docs, model, batch_size=15, wait_time=60):\n",
        "    embedded_docs = []\n",
        "    all_vectors = []\n",
        "\n",
        "    for i in range(0, len(docs), batch_size):\n",
        "        batch = docs[i:i + batch_size]\n",
        "        contents = [doc.page_content for doc in batch]\n",
        "\n",
        "        print(f\"Embedding batch {i // batch_size + 1} / {len(docs) // batch_size + 1}\")\n",
        "        try:\n",
        "            embeddings = model.embed_documents(contents)\n",
        "            # Normalize each embedding to unit vector (cosine similarity)\n",
        "            norm_embeddings = normalize(embeddings, axis=1)\n",
        "\n",
        "            for doc, norm_emb in zip(batch, norm_embeddings):\n",
        "                all_vectors.append(norm_emb)\n",
        "                embedded_docs.append(doc)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error embedding batch {i}: {e}\")\n",
        "\n",
        "        if i + batch_size < len(docs):\n",
        "            time.sleep(wait_time)  # Respect rate limit\n",
        "\n",
        "    return embedded_docs, np.array(all_vectors)"
      ],
      "metadata": {
        "id": "ZBg5r3JN8G-p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Build and save FAISS vectorstore ===\n",
        "def build_faiss_vectorstore(docs, vectors, embedding_model, save_path=\"resume_vectorstore\"):\n",
        "    # Create the list of tuples (text, embedding)\n",
        "    text_embeddings = [(doc.page_content, vectors[i].tolist()) for i, doc in enumerate(docs)]\n",
        "    faiss_store = FAISS.from_embeddings(text_embeddings=text_embeddings, embedding=embedding_model)\n",
        "    faiss_store.save_local(save_path)\n",
        "    print(f\"âœ… FAISS vectorstore saved at: {save_path}\")"
      ],
      "metadata": {
        "id": "70G_zwxo8KQj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === MAIN Pipeline ===\n",
        "def process_resume(pdf_path):\n",
        "    print(\"ğŸ“„ Extracting text from PDF...\")\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    #print(text)\n",
        "\n",
        "    print(\"ğŸ”— Extracting links (e.g., GitHub, LinkedIn)...\")\n",
        "    links = extract_links_from_pdf(pdf_path)\n",
        "    for t, u in links:\n",
        "        print(f\"   â€¢ {t} â†’ {u}\")\n",
        "\n",
        "    print(\"âœ‚ï¸ Chunking with metadata...\")\n",
        "    docs = chunk_text_with_metadata(text, links)\n",
        "    #print(docs)\n",
        "\n",
        "    print(\"ğŸ”— Embedding and normalizing with Gemini...\")\n",
        "    # Pass the API key directly to the constructor\n",
        "    model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "    embedded_docs, norm_vectors = embed_and_normalize_documents(docs, model)\n",
        "    #print(embedded_docs)\n",
        "    #print(norm_vectors)\n",
        "\n",
        "    print(\"ğŸ’¾ Building FAISS vectorstore...\")\n",
        "    build_faiss_vectorstore(embedded_docs, norm_vectors, model)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_resume(\"Resume_fulltime.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RKxdOJq8MuL",
        "outputId": "8214b06c-cf01-431f-81b3-71d526f01360"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“„ Extracting text from PDF...\n",
            "ğŸ”— Extracting links (e.g., GitHub, LinkedIn)...\n",
            "   â€¢ pavan.agni4@gmail.com â†’ mailto:pavan.agni4@gmail.com\n",
            "   â€¢ GitHub â†’ https://github.com/PavanKAgnihotri?tab=repositories\n",
            "   â€¢ LinkedIn â†’ https://www.linkedin.com/in/pavan-k-agnihotri-026908190\n",
            "âœ‚ï¸ Chunking with metadata...\n",
            "ğŸ”— Embedding and normalizing with Gemini...\n",
            "Embedding batch 1 / 1\n",
            "ğŸ’¾ Building FAISS vectorstore...\n",
            "âœ… FAISS vectorstore saved at: resume_vectorstore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Job Description Ingestion Pipeline"
      ],
      "metadata": {
        "id": "xC1t4n3hCpfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Chunk the JD string ===\n",
        "def chunk_job_description(jd_text, chunk_size=800, chunk_overlap=100):\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    return splitter.create_documents([jd_text])"
      ],
      "metadata": {
        "id": "skCkBLOJ9Pzl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Build and save FAISS vectorstore ===\n",
        "def build_faiss_vectorstore_jd(docs, vectors, embedding_model, save_path=\"jd_vectorstore\"):\n",
        "    # Create the list of tuples (text, embedding)\n",
        "    text_embeddings = [(doc.page_content, vectors[i].tolist()) for i, doc in enumerate(docs)]\n",
        "    faiss_store = FAISS.from_embeddings(text_embeddings=text_embeddings, embedding=embedding_model)\n",
        "    faiss_store.save_local(save_path)\n",
        "    print(f\"âœ… FAISS vectorstore saved at: {save_path}\")"
      ],
      "metadata": {
        "id": "OUdowML4EWs2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === MAIN ===\n",
        "def process_jd_string(jd_text: str, save_path=\"jd_vectorstore\"):\n",
        "    print(\"âœ‚ï¸ Chunking JD...\")\n",
        "    docs_jd = chunk_job_description(jd_text)\n",
        "\n",
        "    print(\"ğŸ” Embedding JD...\")\n",
        "    model_jd = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "    embedded_docs_jd, norm_vectors_jd = embed_and_normalize_documents(docs_jd, model_jd)\n",
        "\n",
        "    print(\"ğŸ’¾ Saving JD FAISS vectorstore...\")\n",
        "    build_faiss_vectorstore_jd(embedded_docs_jd, norm_vectors_jd, model_jd)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    jd_input = \"\"\"\n",
        "    AI Engineer\n",
        "    Enterprise AI/ML Organization\n",
        "    Reports to Leader of AI Engineering Group\n",
        "\n",
        "    OVERVIEW\n",
        "    We are seeking a highly skilled and forward-thinking AI Engineer to join our AI Engineering team. This role is ideal for a hands-on technologist with deep expertise in machine learning (ML) and deep learning (DL), and a strong working knowledge of Generative AI technologies. You will design, build, and optimize intelligent systems that power automation, personalization, and decision-making across our FinTech platforms. This is a unique opportunity to work at the intersection of cutting-edge AI research and real-world enterprise applications.\n",
        "\n",
        "    RESPONSIBILITIES\n",
        "    Design, develop, and deploy ML, DL, and Generative AI models that solve complex business problems and deliver measurable value.\n",
        "    Build and maintain scalable ML pipelines and agentic workflows using modern frameworks and cloud-native tools.\n",
        "    Collaborate with data scientists, product managers, and engineers to translate business requirements into AI-powered solutions.\n",
        "    Implement and optimize AI solutions using platforms such as GCP Vertex AI, AWS Bedrock/SageMaker, and Snowflake Cortex.\n",
        "    Apply techniques such as prompt engineering, RAG (Retrieval-Augmented Generation), fine-tuning, and RLHF to enhance model performance.\n",
        "    Develop and deploy autonomous AI agents using frameworks like LangChain, LangGraph, and AgentSpace.\n",
        "    Integrate vector databases (e.g., PGVector) and LLM orchestration tools to support retrieval and memory in generative systems.\n",
        "    Ensure robust MLOps practices including CI/CD, monitoring, versioning, and lifecycle management of models.\n",
        "    Stay current with AI research and industry trends, and evaluate emerging tools and techniques for enterprise adoption.\n",
        "    Contribute to internal knowledge sharing, documentation, and best practices for responsible and ethical AI development.\n",
        "\n",
        "    Must Haves:\n",
        "    Bachelorâ€™s or Masterâ€™s degree in Computer Science, Engineering, or a related field.\n",
        "    4+ years of experience in AI/ML engineering, with a strong foundation in ML/DL algorithms and systems.\n",
        "    Proficiency in Python and ML libraries such as TensorFlow, PyTorch, and Transformers.\n",
        "    Hands-on experience with Generative AI models (e.g., GPT, Mistral, Claude) and agentic AI systems.\n",
        "    Experience with NLP, conversational AI, and LLM-based applications.\n",
        "    Familiarity with vector search and semantic retrieval technologies.\n",
        "    Strong understanding of MLOps, including model deployment, monitoring, and retraining.\n",
        "    Experience building production-grade AI systems at scale in cloud environments.\n",
        "    Excellent problem-solving, communication, and collaboration skills.\n",
        "    \"\"\"\n",
        "    process_jd_string(jd_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtLfQNj7C2rF",
        "outputId": "25ddbe0e-956a-4b95-e4bd-45cc67ba478e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ‚ï¸ Chunking JD...\n",
            "ğŸ” Embedding JD...\n",
            "ğŸ”„ Embedding JD batch 1 / 1\n",
            "ğŸ’¾ Saving JD FAISS vectorstore...\n",
            "âœ… FAISS vectorstore saved at: jd_vectorstore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Semantic Matching Engine"
      ],
      "metadata": {
        "id": "YUF-fwaAFjIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "Fl0xW6ufFgr7"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load vectorstores ===\n",
        "def load_vectorstores(resume_path=\"resume_vectorstore\", jd_path=\"jd_vectorstore\"):\n",
        "    print(\"ğŸ“¥ Loading vectorstores...\")\n",
        "    # Set allow_dangerous_deserialization to True since we trust the source (locally saved)\n",
        "    resume_store = FAISS.load_local(resume_path, embedding_model, allow_dangerous_deserialization=True)\n",
        "    jd_store = FAISS.load_local(jd_path, embedding_model, allow_dangerous_deserialization=True)\n",
        "\n",
        "    resume_docs = resume_store.docstore._dict\n",
        "    jd_docs = jd_store.docstore._dict\n",
        "\n",
        "    resume_vectors = resume_store.index.reconstruct_n(0, resume_store.index.ntotal)\n",
        "    jd_vectors = jd_store.index.reconstruct_n(0, jd_store.index.ntotal)\n",
        "\n",
        "    return list(resume_docs.values()), list(jd_docs.values()), resume_vectors, jd_vectors"
      ],
      "metadata": {
        "id": "-CaSRjLKEeGT"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Compute cosine similarity matrix ===\n",
        "def compute_similarity_matrix(resume_vectors, jd_vectors):\n",
        "    print(\"ğŸ§  Computing cosine similarity...\")\n",
        "    sim_matrix = np.dot(jd_vectors, resume_vectors.T)  # dot product of normalized vectors = cosine similarity\n",
        "    return sim_matrix"
      ],
      "metadata": {
        "id": "FHiiV73fFp0F"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Match JD chunks to Resume chunks ===\n",
        "def rank_resume_matches(jd_docs: List[Document], resume_docs: List[Document], sim_matrix: np.ndarray, top_k=3):\n",
        "    results = []\n",
        "\n",
        "    for i, jd_doc in enumerate(jd_docs):\n",
        "        jd_text = jd_doc.page_content.strip()\n",
        "        sims = sim_matrix[i]\n",
        "        top_indices = np.argsort(sims)[::-1][:top_k]  # descending order\n",
        "\n",
        "        top_matches = []\n",
        "        for idx in top_indices:\n",
        "            match = {\n",
        "                \"resume_chunk\": resume_docs[idx].page_content.strip(),\n",
        "                \"similarity\": round(float(sims[idx]), 4),\n",
        "                \"links\": resume_docs[idx].metadata.get(\"links\", [])\n",
        "            }\n",
        "            top_matches.append(match)\n",
        "\n",
        "        results.append({\n",
        "            \"jd_chunk\": jd_text,\n",
        "            \"top_resume_matches\": top_matches\n",
        "        })\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "xkcQFCEnFr3u"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Display Matches ===\n",
        "def print_matches(matches):\n",
        "    for idx, result in enumerate(matches):\n",
        "        print(f\"\\nğŸ“Œ JD Chunk {idx+1}: {result['jd_chunk']}\\n\")\n",
        "        for match in result[\"top_resume_matches\"]:\n",
        "            print(f\"   ğŸ”¹ Similarity: {match['similarity']}\")\n",
        "            print(f\"   ğŸ“„ Resume Match:\\n   {match['resume_chunk']}\")\n",
        "            if match['links']:\n",
        "                print(f\"   ğŸ”— Links: {match['links']}\")\n",
        "            print(\"   ---\")"
      ],
      "metadata": {
        "id": "5T0LCSMiF2Oz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Resume Rewriter + Humanizer using Gemini (LLM)."
      ],
      "metadata": {
        "id": "WLgDAqmQIHzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model_rewrite = genai.GenerativeModel(\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "FpXR3qZtGDg7"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Prompt template ===\n",
        "def build_prompt(jd_chunk: str, resume_chunk: str, links=None):\n",
        "    links_str = \"\"\n",
        "    if links:\n",
        "        links_str = \"\\n\\nRelevant links:\\n\" + \"\\n\".join([f\"{text}: {url}\" for text, url in links])\n",
        "\n",
        "    return f\"\"\"\n",
        "            You are a resume rewriting assistant.\n",
        "\n",
        "            Given the job requirement below and the resume content, rewrite the resume content to:\n",
        "            - Match the job requirement more strongly\n",
        "            - Use modern, professional, and natural-sounding language\n",
        "            - Keep it in bullet point format\n",
        "            - Include relevant links if they add value\n",
        "\n",
        "            Job Description:\n",
        "            {jd_chunk}\n",
        "\n",
        "            Original Resume Bullet:\n",
        "            {resume_chunk}\n",
        "\n",
        "            {links_str}\n",
        "\n",
        "            Return only the rewritten bullet.\n",
        "            \"\"\""
      ],
      "metadata": {
        "id": "RbbnQtpyIL7f"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Rewrite function ===\n",
        "def rewrite_resume_matches(matches, rate_limit=15):\n",
        "    rewritten_chunks = []\n",
        "    call_count = 0\n",
        "\n",
        "    for i, match_group in enumerate(matches):\n",
        "        jd_chunk = match_group[\"jd_chunk\"]\n",
        "        for match in match_group[\"top_resume_matches\"]:\n",
        "            if call_count >= rate_limit:\n",
        "                print(\"âš ï¸ Rate limit reached. Sleeping for 60 seconds...\")\n",
        "                time.sleep(60)\n",
        "                call_count = 0\n",
        "\n",
        "            prompt = build_prompt(jd_chunk, match[\"resume_chunk\"], match.get(\"links\", []))\n",
        "            try:\n",
        "                response = model_rewrite.generate_content(prompt)\n",
        "                rewritten = response.text.strip()\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error rewriting chunk: {e}\")\n",
        "                rewritten = match[\"resume_chunk\"]\n",
        "\n",
        "            rewritten_chunks.append({\n",
        "                \"original\": match[\"resume_chunk\"],\n",
        "                \"rewritten\": rewritten,\n",
        "                \"jd_context\": jd_chunk,\n",
        "                \"similarity\": match[\"similarity\"],\n",
        "                \"links\": match.get(\"links\", [])\n",
        "            })\n",
        "\n",
        "            call_count += 1\n",
        "\n",
        "    return rewritten_chunks"
      ],
      "metadata": {
        "id": "I1KeYsQCIUYi"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === MAIN Runner ===\n",
        "def match_resume_to_jd():\n",
        "    resume_docs, jd_docs, resume_vectors, jd_vectors = load_vectorstores()\n",
        "    sim_matrix = compute_similarity_matrix(resume_vectors, jd_vectors)\n",
        "    matches = rank_resume_matches(jd_docs, resume_docs, sim_matrix, top_k=3)\n",
        "    #print_matches(matches)\n",
        "    print(\"âœï¸ Rewriting best-matched resume chunks...\")\n",
        "    rewritten = rewrite_resume_matches(matches)\n",
        "\n",
        "    for item in rewritten:\n",
        "        print(\"\\n---------------------------\")\n",
        "        print(f\"ğŸ”¹ JD Context:\\n{item['jd_context']}\")\n",
        "        print(f\"ğŸ”¸ Original:\\n{item['original']}\")\n",
        "        print(f\"âœ… Rewritten:\\n{item['rewritten']}\")\n",
        "        if item[\"links\"]:\n",
        "            print(f\"ğŸ”— Links: {item['links']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    match_resume_to_jd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3y5kk0EzF6uE",
        "outputId": "fe87b457-12d3-4eb6-c688-b3148099e39b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¥ Loading vectorstores...\n",
            "ğŸ§  Computing cosine similarity...\n",
            "âœï¸ Rewriting best-matched resume chunks...\n",
            "\n",
            "---------------------------\n",
            "ğŸ”¹ JD Context:\n",
            "AI Engineer\n",
            "    Enterprise AI/ML Organization\n",
            "    Reports to Leader of AI Engineering Group\n",
            "\n",
            "    OVERVIEW\n",
            "    We are seeking a highly skilled and forward-thinking AI Engineer to join our AI Engineering team. This role is ideal for a hands-on technologist with deep expertise in machine learning (ML) and deep learning (DL), and a strong working knowledge of Generative AI technologies. You will design, build, and optimize intelligent systems that power automation, personalization, and decision-making across our FinTech platforms. This is a unique opportunity to work at the intersection of cutting-edge AI research and real-world enterprise applications.\n",
            "ğŸ”¸ Original:\n",
            "â€¢ Implemented a Generative AI model with prompt engineering to assess document grammar, structure, and identify missing information\n",
            "in paragraphs.\n",
            "Tech Fortune Technologies Jul 2019 - Aug 2019\n",
            "Machine Learning Intern Bengaluru, India\n",
            "â€¢ Explored various Machine Learning models and their applications, reinforcing a proactive learning approach.\n",
            "â€¢ Applied a Machine Learning model to predict protein and carbohydrate content in diets with 80% accuracy, integrating practical\n",
            "software implementation skills.\n",
            "ACADEMIC PROJECTS\n",
            "RAG Chatbot using Gemini API Nov 2024\n",
            "University of Alabama at Birmingham Birmingham, AL, US\n",
            "â€¢ Developed a Retrieval-Augmented Generation (RAG) chatbot using the Gemini API, leveraging data from an Excel sheet for context-\n",
            "aware responses.\n",
            "âœ… Rewritten:\n",
            "* **Developed and deployed a Generative AI model (Tech Fortune Technologies, Summer 2019):**  Engineered prompts to assess document grammar, structure, and identify missing information within paragraphs, demonstrating proficiency in generative AI applications.\n",
            "\n",
            "* **Developed a Retrieval-Augmented Generation (RAG) Chatbot (University of Alabama at Birmingham, Nov 2024):**  Leveraged the Gemini API and Excel-based data to build a context-aware chatbot, showcasing expertise in large language model integration and data processing.\n",
            "\n",
            "* **Applied Machine Learning for Dietary Analysis (Tech Fortune Technologies, Summer 2019):**  Successfully implemented a machine learning model achieving 80% accuracy in predicting protein and carbohydrate content in diets, demonstrating practical software implementation skills and model selection expertise.\n",
            "\n",
            "* **Proactive and Continuous Learning:** Consistently explored various machine learning models and their applications, fostering a proactive approach to skill development and staying current with advancements in the field.\n",
            "\n",
            "---------------------------\n",
            "ğŸ”¹ JD Context:\n",
            "AI Engineer\n",
            "    Enterprise AI/ML Organization\n",
            "    Reports to Leader of AI Engineering Group\n",
            "\n",
            "    OVERVIEW\n",
            "    We are seeking a highly skilled and forward-thinking AI Engineer to join our AI Engineering team. This role is ideal for a hands-on technologist with deep expertise in machine learning (ML) and deep learning (DL), and a strong working knowledge of Generative AI technologies. You will design, build, and optimize intelligent systems that power automation, personalization, and decision-making across our FinTech platforms. This is a unique opportunity to work at the intersection of cutting-edge AI research and real-world enterprise applications.\n",
            "ğŸ”¸ Original:\n",
            "EDUCATION\n",
            "University of Alabama at Birmingham Aug 2024 - Dec 2025\n",
            "Master of Science, Computer Science\n",
            "â€¢ GPA: 3.9\n",
            "BNM Institute of Technology Aug 2016 - Aug 2020\n",
            "Bachelor of Engineering, Computer Science & Engineering\n",
            "â€¢ GPA: 3.49\n",
            "WORK EXPERIENCE\n",
            "Accenture Solutions Private Limited Jan 2021 - Jul 2024\n",
            "Custom Software Engineering Analyst Bengaluru, India\n",
            "â€¢ Designed and developed Python applications to send user reminders for review tasks, ensuring clean and maintainable code.\n",
            "â€¢ Automated the Quarterly Review Process using Python and Flask, enhancing process efficiency and reliability.\n",
            "â€¢ Engineered dashboards with the ELK stack to monitor system availability, CPU/memory utilization, and database performance.\n",
            "âœ… Rewritten:\n",
            "* **Master of Science in Computer Science, University of Alabama at Birmingham (Expected December 2025):**  GPA 3.9.  Currently pursuing advanced coursework in machine learning and deep learning, with a focus on generative AI models.\n",
            "\n",
            "* **Bachelor of Engineering in Computer Science & Engineering, BNM Institute of Technology (2016-2020):** GPA 3.49.\n",
            "\n",
            "* **Custom Software Engineering Analyst, Accenture Solutions Private Limited (January 2021 â€“ July 2024):**  Developed and deployed Python applications leveraging Flask for task automation and user engagement, resulting in enhanced efficiency and reliability in the quarterly review process.  Engineered and implemented ELK stack-based dashboards for comprehensive monitoring of system performance metrics, including CPU/memory utilization and database performance, ensuring high system availability.  Emphasized clean, maintainable code throughout all projects.\n",
            "\n",
            "---------------------------\n",
            "ğŸ”¹ JD Context:\n",
            "AI Engineer\n",
            "    Enterprise AI/ML Organization\n",
            "    Reports to Leader of AI Engineering Group\n",
            "\n",
            "    OVERVIEW\n",
            "    We are seeking a highly skilled and forward-thinking AI Engineer to join our AI Engineering team. This role is ideal for a hands-on technologist with deep expertise in machine learning (ML) and deep learning (DL), and a strong working knowledge of Generative AI technologies. You will design, build, and optimize intelligent systems that power automation, personalization, and decision-making across our FinTech platforms. This is a unique opportunity to work at the intersection of cutting-edge AI research and real-world enterprise applications.\n",
            "ğŸ”¸ Original:\n",
            "Machine Learning\n",
            "HONORS & AWARDS\n",
            "â€¢ Certificate of Recognition: Certificate of Recognition for outstanding performance and contributions to the GSK project - October\n",
            "2022\n",
            "â€¢ APEX Award: APEX Award for exceptional effort and exemplary work - March 2023\n",
            "âœ… Rewritten:\n",
            "* **Developed and implemented machine learning models** contributing to the GSK project, recognized with a Certificate of Recognition in October 2022 for outstanding performance.  *Further details available upon request.*\n",
            "* Achieved an APEX Award in March 2023 for exceptional work and contributions.\n",
            "\n",
            "---------------------------\n",
            "ğŸ”¹ JD Context:\n",
            "RESPONSIBILITIES\n",
            "    Design, develop, and deploy ML, DL, and Generative AI models that solve complex business problems and deliver measurable value.\n",
            "    Build and maintain scalable ML pipelines and agentic workflows using modern frameworks and cloud-native tools.\n",
            "    Collaborate with data scientists, product managers, and engineers to translate business requirements into AI-powered solutions.\n",
            "    Implement and optimize AI solutions using platforms such as GCP Vertex AI, AWS Bedrock/SageMaker, and Snowflake Cortex.\n",
            "    Apply techniques such as prompt engineering, RAG (Retrieval-Augmented Generation), fine-tuning, and RLHF to enhance model performance.\n",
            "    Develop and deploy autonomous AI agents using frameworks like LangChain, LangGraph, and AgentSpace.\n",
            "ğŸ”¸ Original:\n",
            "â€¢ Implemented a Generative AI model with prompt engineering to assess document grammar, structure, and identify missing information\n",
            "in paragraphs.\n",
            "Tech Fortune Technologies Jul 2019 - Aug 2019\n",
            "Machine Learning Intern Bengaluru, India\n",
            "â€¢ Explored various Machine Learning models and their applications, reinforcing a proactive learning approach.\n",
            "â€¢ Applied a Machine Learning model to predict protein and carbohydrate content in diets with 80% accuracy, integrating practical\n",
            "software implementation skills.\n",
            "ACADEMIC PROJECTS\n",
            "RAG Chatbot using Gemini API Nov 2024\n",
            "University of Alabama at Birmingham Birmingham, AL, US\n",
            "â€¢ Developed a Retrieval-Augmented Generation (RAG) chatbot using the Gemini API, leveraging data from an Excel sheet for context-\n",
            "aware responses.\n",
            "âœ… Rewritten:\n",
            "* Developed and deployed a generative AI model using prompt engineering to analyze document grammar, structure, and identify missing information, improving the overall quality of textual content. (Tech Fortune Technologies, Machine Learning Intern, Bengaluru, India, Jul 2019 - Aug 2019)\n",
            "\n",
            "* Designed and implemented a machine learning model achieving 80% accuracy in predicting protein and carbohydrate content in diets, demonstrating practical software implementation skills. (Tech Fortune Technologies, Machine Learning Intern, Bengaluru, India, Jul 2019 - Aug 2019)\n",
            "\n",
            "* Built a Retrieval-Augmented Generation (RAG) chatbot leveraging the Gemini API and an Excel sheet as a knowledge base to deliver context-aware responses. (University of Alabama at Birmingham, Academic Project, Nov 2024)\n",
            "\n",
            "---------------------------\n",
            "ğŸ”¹ JD Context:\n",
            "RESPONSIBILITIES\n",
            "    Design, develop, and deploy ML, DL, and Generative AI models that solve complex business problems and deliver measurable value.\n",
            "    Build and maintain scalable ML pipelines and agentic workflows using modern frameworks and cloud-native tools.\n",
            "    Collaborate with data scientists, product managers, and engineers to translate business requirements into AI-powered solutions.\n",
            "    Implement and optimize AI solutions using platforms such as GCP Vertex AI, AWS Bedrock/SageMaker, and Snowflake Cortex.\n",
            "    Apply techniques such as prompt engineering, RAG (Retrieval-Augmented Generation), fine-tuning, and RLHF to enhance model performance.\n",
            "    Develop and deploy autonomous AI agents using frameworks like LangChain, LangGraph, and AgentSpace.\n",
            "ğŸ”¸ Original:\n",
            "aware responses.\n",
            "YouTube Comments Spam detection using Machine Learning Aug 2024 - Nov 2024\n",
            "University of Alabama at Birmingham Birmingham, AL, US\n",
            "â€¢ Developed a spam detection system for YouTube comments using many different ML models\n",
            "Hotel Recommendation System using Machine Learning Jan 2020 - Jun 2020\n",
            "BNM Institute of Technology Bengaluru, India\n",
            "â€¢ Developed a hotel recommendation system using Machine Learning that suggests hotels based on user comments and\n",
            "ratings.\n",
            "TECHNICAL SKILLS\n",
            "â€¢ Programming Language: Python, Flask, HTML & CSS, MySQL, ELK Stack, PyTorch\n",
            "â€¢ Microsoft Office Package: Word, Excel, PowerPoint, PowerApps, Power Automate, Copilot Studio\n",
            "â€¢ Technical Skills: Data Structures, Algorithms, Database Management, Artificial Intelligence, Web Development Frameworks,\n",
            "âœ… Rewritten:\n",
            "* **YouTube Comment Spam Detection (Aug 2024 â€“ Nov 2024), University of Alabama at Birmingham, Birmingham, AL:** Developed and deployed a machine learning-based spam detection system for YouTube comments, leveraging multiple ML models to achieve high accuracy and efficiency.  [*(Optional: Add a link to a GitHub repository or project portfolio if available)*]\n",
            "\n",
            "* **Hotel Recommendation System (Jan 2020 â€“ Jun 2020), BNM Institute of Technology, Bengaluru, India:** Designed and implemented a machine learning-powered hotel recommendation system that analyzes user comments and ratings to provide personalized suggestions.  [*(Optional: Add a link to a GitHub repository or project portfolio if available)*]\n",
            "\n",
            "---------------------------\n",
            "ğŸ”¹ JD Context:\n",
            "RESPONSIBILITIES\n",
            "    Design, develop, and deploy ML, DL, and Generative AI models that solve complex business problems and deliver measurable value.\n",
            "    Build and maintain scalable ML pipelines and agentic workflows using modern frameworks and cloud-native tools.\n",
            "    Collaborate with data scientists, product managers, and engineers to translate business requirements into AI-powered solutions.\n",
            "    Implement and optimize AI solutions using platforms such as GCP Vertex AI, AWS Bedrock/SageMaker, and Snowflake Cortex.\n",
            "    Apply techniques such as prompt engineering, RAG (Retrieval-Augmented Generation), fine-tuning, and RLHF to enhance model performance.\n",
            "    Develop and deploy autonomous AI agents using frameworks like LangChain, LangGraph, and AgentSpace.\n",
            "ğŸ”¸ Original:\n",
            "Machine Learning\n",
            "HONORS & AWARDS\n",
            "â€¢ Certificate of Recognition: Certificate of Recognition for outstanding performance and contributions to the GSK project - October\n",
            "2022\n",
            "â€¢ APEX Award: APEX Award for exceptional effort and exemplary work - March 2023\n",
            "âœ… Rewritten:\n",
            "* Developed and deployed machine learning models, achieving [quantifiable result, e.g., a 15% increase in efficiency] on the GSK project (recognized with a Certificate of Recognition, October 2022).  \n",
            "* Exceeded expectations on a key project, earning the APEX Award for exceptional performance (March 2023).\n",
            "\n",
            "---------------------------\n",
            "ğŸ”¹ JD Context:\n",
            "Integrate vector databases (e.g., PGVector) and LLM orchestration tools to support retrieval and memory in generative systems.\n",
            "    Ensure robust MLOps practices including CI/CD, monitoring, versioning, and lifecycle management of models.\n",
            "    Stay current with AI research and industry trends, and evaluate emerging tools and techniques for enterprise adoption.\n",
            "    Contribute to internal knowledge sharing, documentation, and best practices for responsible and ethical AI development.\n",
            "ğŸ”¸ Original:\n",
            "â€¢ Implemented a Generative AI model with prompt engineering to assess document grammar, structure, and identify missing information\n",
            "in paragraphs.\n",
            "Tech Fortune Technologies Jul 2019 - Aug 2019\n",
            "Machine Learning Intern Bengaluru, India\n",
            "â€¢ Explored various Machine Learning models and their applications, reinforcing a proactive learning approach.\n",
            "â€¢ Applied a Machine Learning model to predict protein and carbohydrate content in diets with 80% accuracy, integrating practical\n",
            "software implementation skills.\n",
            "ACADEMIC PROJECTS\n",
            "RAG Chatbot using Gemini API Nov 2024\n",
            "University of Alabama at Birmingham Birmingham, AL, US\n",
            "â€¢ Developed a Retrieval-Augmented Generation (RAG) chatbot using the Gemini API, leveraging data from an Excel sheet for context-\n",
            "aware responses.\n",
            "âœ… Rewritten:\n",
            "* Developed a Retrieval-Augmented Generation (RAG) chatbot utilizing the Gemini API and an Excel sheet data source, demonstrating proficiency in integrating LLMs with external knowledge bases for context-aware responses.  This project showcased practical experience in building generative systems with external knowledge retrieval.\n",
            "* Implemented a generative AI model incorporating prompt engineering techniques to analyze document grammar, structure, and identify missing information, showcasing skills applicable to enhancing the accuracy and quality of LLM outputs.\n",
            "* Applied machine learning models to predict dietary macronutrient content (protein and carbohydrate) achieving 80% accuracy, demonstrating practical software implementation skills and a data-driven approach relevant to model development and deployment.  (Note:  Consider adding a brief mention of the specific ML model used if it's relevant to vector databases or LLMs).\n",
            "\n",
            "---------------------------\n",
            "ğŸ”¹ JD Context:\n",
            "Integrate vector databases (e.g., PGVector) and LLM orchestration tools to support retrieval and memory in generative systems.\n",
            "    Ensure robust MLOps practices including CI/CD, monitoring, versioning, and lifecycle management of models.\n",
            "    Stay current with AI research and industry trends, and evaluate emerging tools and techniques for enterprise adoption.\n",
            "    Contribute to internal knowledge sharing, documentation, and best practices for responsible and ethical AI development.\n",
            "ğŸ”¸ Original:\n",
            "Machine Learning\n",
            "HONORS & AWARDS\n",
            "â€¢ Certificate of Recognition: Certificate of Recognition for outstanding performance and contributions to the GSK project - October\n",
            "2022\n",
            "â€¢ APEX Award: APEX Award for exceptional effort and exemplary work - March 2023\n",
            "âœ… Rewritten:\n",
            "* Developed and implemented machine learning models, contributing to significant project successes as evidenced by a Certificate of Recognition (October 2022, GSK project) and an APEX Award (March 2023) for exceptional performance.  (Further details available upon request)\n",
            "\n",
            "---------------------------\n",
            "ğŸ”¹ JD Context:\n",
            "Integrate vector databases (e.g., PGVector) and LLM orchestration tools to support retrieval and memory in generative systems.\n",
            "    Ensure robust MLOps practices including CI/CD, monitoring, versioning, and lifecycle management of models.\n",
            "    Stay current with AI research and industry trends, and evaluate emerging tools and techniques for enterprise adoption.\n",
            "    Contribute to internal knowledge sharing, documentation, and best practices for responsible and ethical AI development.\n",
            "ğŸ”¸ Original:\n",
            "aware responses.\n",
            "YouTube Comments Spam detection using Machine Learning Aug 2024 - Nov 2024\n",
            "University of Alabama at Birmingham Birmingham, AL, US\n",
            "â€¢ Developed a spam detection system for YouTube comments using many different ML models\n",
            "Hotel Recommendation System using Machine Learning Jan 2020 - Jun 2020\n",
            "BNM Institute of Technology Bengaluru, India\n",
            "â€¢ Developed a hotel recommendation system using Machine Learning that suggests hotels based on user comments and\n",
            "ratings.\n",
            "TECHNICAL SKILLS\n",
            "â€¢ Programming Language: Python, Flask, HTML & CSS, MySQL, ELK Stack, PyTorch\n",
            "â€¢ Microsoft Office Package: Word, Excel, PowerPoint, PowerApps, Power Automate, Copilot Studio\n",
            "â€¢ Technical Skills: Data Structures, Algorithms, Database Management, Artificial Intelligence, Web Development Frameworks,\n",
            "âœ… Rewritten:\n",
            "* **Developed and deployed a YouTube comment spam detection system (Aug 2024 - Nov 2024, University of Alabama at Birmingham):**  Utilized multiple machine learning models to identify and filter spam, improving platform integrity.  [Optional: Add a link to a portfolio showcasing this project if available]\n",
            "\n",
            "* **Designed and implemented a hotel recommendation engine (Jan 2020 - Jun 2020, BNM Institute of Technology):** Leveraged machine learning algorithms to personalize hotel suggestions based on user reviews and ratings, enhancing user experience. [Optional: Add a link to a portfolio showcasing this project if available]\n",
            "\n",
            "* **Proficient in Python, including relevant libraries for machine learning (PyTorch) and web development (Flask).**  Experienced with database management (MySQL) and data visualization tools (ELK Stack).\n",
            "\n",
            "* **Familiar with MLOps principles including CI/CD and model versioning.**  Eager to apply these skills to the integration of vector databases and LLM orchestration tools for generative AI systems.\n",
            "\n",
            "---------------------------\n",
            "ğŸ”¹ JD Context:\n",
            "Must Haves:\n",
            "    Bachelorâ€™s or Masterâ€™s degree in Computer Science, Engineering, or a related field.\n",
            "    4+ years of experience in AI/ML engineering, with a strong foundation in ML/DL algorithms and systems.\n",
            "    Proficiency in Python and ML libraries such as TensorFlow, PyTorch, and Transformers.\n",
            "    Hands-on experience with Generative AI models (e.g., GPT, Mistral, Claude) and agentic AI systems.\n",
            "    Experience with NLP, conversational AI, and LLM-based applications.\n",
            "    Familiarity with vector search and semantic retrieval technologies.\n",
            "    Strong understanding of MLOps, including model deployment, monitoring, and retraining.\n",
            "    Experience building production-grade AI systems at scale in cloud environments.\n",
            "    Excellent problem-solving, communication, and collaboration skills.\n",
            "ğŸ”¸ Original:\n",
            "â€¢ Implemented a Generative AI model with prompt engineering to assess document grammar, structure, and identify missing information\n",
            "in paragraphs.\n",
            "Tech Fortune Technologies Jul 2019 - Aug 2019\n",
            "Machine Learning Intern Bengaluru, India\n",
            "â€¢ Explored various Machine Learning models and their applications, reinforcing a proactive learning approach.\n",
            "â€¢ Applied a Machine Learning model to predict protein and carbohydrate content in diets with 80% accuracy, integrating practical\n",
            "software implementation skills.\n",
            "ACADEMIC PROJECTS\n",
            "RAG Chatbot using Gemini API Nov 2024\n",
            "University of Alabama at Birmingham Birmingham, AL, US\n",
            "â€¢ Developed a Retrieval-Augmented Generation (RAG) chatbot using the Gemini API, leveraging data from an Excel sheet for context-\n",
            "aware responses.\n",
            "âœ… Rewritten:\n",
            "* **Developed a Retrieval-Augmented Generation (RAG) chatbot using Google's Gemini API:**  Leveraged an Excel spreadsheet as a knowledge base to create context-aware responses, demonstrating proficiency in LLM-based application development and NLP techniques.  [Optional: Add a link to a portfolio showcasing the project if available]\n",
            "* **Implemented a generative AI model for document analysis at Tech Fortune Technologies:** Employed prompt engineering techniques to assess document grammar, structure, and identify missing information within paragraphs, showcasing practical experience with generative AI model implementation.\n",
            "* **Applied machine learning models for dietary analysis at Tech Fortune Technologies:** Developed and implemented a machine learning model achieving 80% accuracy in predicting protein and carbohydrate content in diets, demonstrating practical software implementation skills and a proficiency in applying ML algorithms to real-world problems.\n",
            "* **Independently explored and evaluated various machine learning models and their applications at Tech Fortune Technologies:** Proactively expanded knowledge of machine learning algorithms and their practical applications, showcasing a commitment to continuous learning and professional development.\n",
            "\n",
            "---------------------------\n",
            "ğŸ”¹ JD Context:\n",
            "Must Haves:\n",
            "    Bachelorâ€™s or Masterâ€™s degree in Computer Science, Engineering, or a related field.\n",
            "    4+ years of experience in AI/ML engineering, with a strong foundation in ML/DL algorithms and systems.\n",
            "    Proficiency in Python and ML libraries such as TensorFlow, PyTorch, and Transformers.\n",
            "    Hands-on experience with Generative AI models (e.g., GPT, Mistral, Claude) and agentic AI systems.\n",
            "    Experience with NLP, conversational AI, and LLM-based applications.\n",
            "    Familiarity with vector search and semantic retrieval technologies.\n",
            "    Strong understanding of MLOps, including model deployment, monitoring, and retraining.\n",
            "    Experience building production-grade AI systems at scale in cloud environments.\n",
            "    Excellent problem-solving, communication, and collaboration skills.\n",
            "ğŸ”¸ Original:\n",
            "aware responses.\n",
            "YouTube Comments Spam detection using Machine Learning Aug 2024 - Nov 2024\n",
            "University of Alabama at Birmingham Birmingham, AL, US\n",
            "â€¢ Developed a spam detection system for YouTube comments using many different ML models\n",
            "Hotel Recommendation System using Machine Learning Jan 2020 - Jun 2020\n",
            "BNM Institute of Technology Bengaluru, India\n",
            "â€¢ Developed a hotel recommendation system using Machine Learning that suggests hotels based on user comments and\n",
            "ratings.\n",
            "TECHNICAL SKILLS\n",
            "â€¢ Programming Language: Python, Flask, HTML & CSS, MySQL, ELK Stack, PyTorch\n",
            "â€¢ Microsoft Office Package: Word, Excel, PowerPoint, PowerApps, Power Automate, Copilot Studio\n",
            "â€¢ Technical Skills: Data Structures, Algorithms, Database Management, Artificial Intelligence, Web Development Frameworks,\n",
            "âœ… Rewritten:\n",
            "* **YouTube Comment Spam Detection (Aug 2024 â€“ Nov 2024), University of Alabama at Birmingham, Birmingham, AL:** Developed and deployed a machine learning-based spam detection system for YouTube comments, leveraging multiple ML models to identify and filter inappropriate content.  Improved accuracy and efficiency of content moderation.\n",
            "\n",
            "* **Hotel Recommendation System (Jan 2020 â€“ Jun 2020), BNM Institute of Technology, Bengaluru, India:** Designed and implemented a machine learning-powered hotel recommendation system. This system analyzed user comments and ratings to provide personalized hotel suggestions, improving user experience and engagement. Utilized [Specify ML algorithms used, e.g., collaborative filtering, content-based filtering].\n",
            "\n",
            "* **Proficient in Python and ML Libraries:** Extensive experience utilizing Python, PyTorch, and TensorFlow for the development and deployment of machine learning models.  Familiar with Transformer architectures.\n",
            "\n",
            "* **Generative AI and LLM Experience:** Hands-on experience with Generative AI models such as GPT, Mistral, and Claude, and a strong understanding of their applications in building LLM-based applications.\n",
            "\n",
            "* **NLP and Conversational AI Expertise:**  Experience in Natural Language Processing (NLP) techniques and their application to building conversational AI systems.\n",
            "\n",
            "\n",
            "* **MLOps and Production Deployment:**  Practical experience in MLOps principles including model deployment, monitoring, retraining, and scaling AI systems in cloud environments (Please specify cloud providers used, e.g., AWS, Azure, GCP).\n",
            "\n",
            "* **Vector Search and Semantic Retrieval:**  Working knowledge of vector search and semantic retrieval technologies for efficient information retrieval.\n",
            "\n",
            "---------------------------\n",
            "ğŸ”¹ JD Context:\n",
            "Must Haves:\n",
            "    Bachelorâ€™s or Masterâ€™s degree in Computer Science, Engineering, or a related field.\n",
            "    4+ years of experience in AI/ML engineering, with a strong foundation in ML/DL algorithms and systems.\n",
            "    Proficiency in Python and ML libraries such as TensorFlow, PyTorch, and Transformers.\n",
            "    Hands-on experience with Generative AI models (e.g., GPT, Mistral, Claude) and agentic AI systems.\n",
            "    Experience with NLP, conversational AI, and LLM-based applications.\n",
            "    Familiarity with vector search and semantic retrieval technologies.\n",
            "    Strong understanding of MLOps, including model deployment, monitoring, and retraining.\n",
            "    Experience building production-grade AI systems at scale in cloud environments.\n",
            "    Excellent problem-solving, communication, and collaboration skills.\n",
            "ğŸ”¸ Original:\n",
            "EDUCATION\n",
            "University of Alabama at Birmingham Aug 2024 - Dec 2025\n",
            "Master of Science, Computer Science\n",
            "â€¢ GPA: 3.9\n",
            "BNM Institute of Technology Aug 2016 - Aug 2020\n",
            "Bachelor of Engineering, Computer Science & Engineering\n",
            "â€¢ GPA: 3.49\n",
            "WORK EXPERIENCE\n",
            "Accenture Solutions Private Limited Jan 2021 - Jul 2024\n",
            "Custom Software Engineering Analyst Bengaluru, India\n",
            "â€¢ Designed and developed Python applications to send user reminders for review tasks, ensuring clean and maintainable code.\n",
            "â€¢ Automated the Quarterly Review Process using Python and Flask, enhancing process efficiency and reliability.\n",
            "â€¢ Engineered dashboards with the ELK stack to monitor system availability, CPU/memory utilization, and database performance.\n",
            "âœ… Rewritten:\n",
            "* **Master of Science in Computer Science**, University of Alabama at Birmingham, Expected Graduation December 2025 (GPA: 3.9)\n",
            "* **Bachelor of Engineering in Computer Science & Engineering**, BNM Institute of Technology, 2016-2020 (GPA: 3.49)\n",
            "* **Custom Software Engineering Analyst**, Accenture Solutions Private Limited, Bengaluru, India (January 2021 â€“ July 2024)\n",
            "    * Developed and deployed Python applications utilizing Flask for user reminder systems, prioritizing code maintainability and scalability.\n",
            "    * Automated the quarterly review process using Python, resulting in significant efficiency gains and improved reliability.\n",
            "    * Engineered and implemented monitoring dashboards using the ELK stack to track system availability, resource utilization (CPU/memory), and database performance.  This provided critical insights for performance optimization and proactive issue resolution.\n",
            "\n",
            "---------------------------\n",
            "ğŸ”¹ JD Context:\n",
            "Excellent problem-solving, communication, and collaboration skills.\n",
            "ğŸ”¸ Original:\n",
            "Machine Learning\n",
            "HONORS & AWARDS\n",
            "â€¢ Certificate of Recognition: Certificate of Recognition for outstanding performance and contributions to the GSK project - October\n",
            "2022\n",
            "â€¢ APEX Award: APEX Award for exceptional effort and exemplary work - March 2023\n",
            "âœ… Rewritten:\n",
            "* **Successfully applied machine learning techniques to contribute significantly to the GSK project, earning a Certificate of Recognition for outstanding performance (October 2022).**\n",
            "* **Achieved exceptional results and demonstrated exemplary work ethic, recognized with the APEX Award (March 2023).**\n",
            "\n",
            "---------------------------\n",
            "ğŸ”¹ JD Context:\n",
            "Excellent problem-solving, communication, and collaboration skills.\n",
            "ğŸ”¸ Original:\n",
            "EDUCATION\n",
            "University of Alabama at Birmingham Aug 2024 - Dec 2025\n",
            "Master of Science, Computer Science\n",
            "â€¢ GPA: 3.9\n",
            "BNM Institute of Technology Aug 2016 - Aug 2020\n",
            "Bachelor of Engineering, Computer Science & Engineering\n",
            "â€¢ GPA: 3.49\n",
            "WORK EXPERIENCE\n",
            "Accenture Solutions Private Limited Jan 2021 - Jul 2024\n",
            "Custom Software Engineering Analyst Bengaluru, India\n",
            "â€¢ Designed and developed Python applications to send user reminders for review tasks, ensuring clean and maintainable code.\n",
            "â€¢ Automated the Quarterly Review Process using Python and Flask, enhancing process efficiency and reliability.\n",
            "â€¢ Engineered dashboards with the ELK stack to monitor system availability, CPU/memory utilization, and database performance.\n",
            "âœ… Rewritten:\n",
            "* **Master of Science in Computer Science, University of Alabama at Birmingham (Expected December 2025):** GPA 3.9.  Focused coursework on [mention relevant coursework if applicable, e.g., software design, data structures, algorithms].\n",
            "* **Bachelor of Engineering in Computer Science & Engineering, BNM Institute of Technology (2016-2020):** GPA 3.49.\n",
            "* **Custom Software Engineering Analyst, Accenture Solutions Private Limited (January 2021 â€“ July 2024):**  Developed and deployed Python applications leveraging Flask to automate the quarterly review process, significantly improving efficiency and reliability.  Engineered and implemented ELK stack-based dashboards for real-time monitoring of system performance, including CPU/memory utilization and database performance, proactively identifying and resolving potential issues.  Designed and implemented a Python-based reminder system for user review tasks, prioritizing code maintainability and readability.\n",
            "\n",
            "---------------------------\n",
            "ğŸ”¹ JD Context:\n",
            "Excellent problem-solving, communication, and collaboration skills.\n",
            "ğŸ”¸ Original:\n",
            "aware responses.\n",
            "YouTube Comments Spam detection using Machine Learning Aug 2024 - Nov 2024\n",
            "University of Alabama at Birmingham Birmingham, AL, US\n",
            "â€¢ Developed a spam detection system for YouTube comments using many different ML models\n",
            "Hotel Recommendation System using Machine Learning Jan 2020 - Jun 2020\n",
            "BNM Institute of Technology Bengaluru, India\n",
            "â€¢ Developed a hotel recommendation system using Machine Learning that suggests hotels based on user comments and\n",
            "ratings.\n",
            "TECHNICAL SKILLS\n",
            "â€¢ Programming Language: Python, Flask, HTML & CSS, MySQL, ELK Stack, PyTorch\n",
            "â€¢ Microsoft Office Package: Word, Excel, PowerPoint, PowerApps, Power Automate, Copilot Studio\n",
            "â€¢ Technical Skills: Data Structures, Algorithms, Database Management, Artificial Intelligence, Web Development Frameworks,\n",
            "âœ… Rewritten:\n",
            "* **YouTube Comment Spam Detection (Machine Learning):**  Developed and implemented a machine learning-based spam detection system for YouTube comments, significantly improving comment moderation efficiency. (Aug 2024 â€“ Nov 2024, University of Alabama at Birmingham, Birmingham, AL)\n",
            "\n",
            "* **Hotel Recommendation System (Machine Learning):** Designed and built a machine learning model for a hotel recommendation system, leveraging user comments and ratings to enhance user experience and engagement. (Jan 2020 â€“ Jun 2020, BNM Institute of Technology, Bengaluru, India)\n",
            "\n",
            "\n",
            "* **Technical Skills:** Proficient in Python (including Flask), HTML, CSS, MySQL, ELK Stack, PyTorch, and data structures and algorithms.  Experienced in database management, artificial intelligence, and web development frameworks.  Expert in Microsoft Office Suite (Word, Excel, PowerPoint, PowerApps, Power Automate, Copilot Studio).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uEbY_K4fIhwF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
